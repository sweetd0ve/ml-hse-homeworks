{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80wasOIL8kya"
      },
      "source": [
        "## 1. Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ypCWHelOP7l6"
      },
      "outputs": [],
      "source": [
        "!pip install pywin32-ctypes==0.2.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3SijFPJnlgPj"
      },
      "outputs": [],
      "source": [
        "# Equivalent to 'import win32api' from pywin32.\n",
        "from win32ctypes.pywin32 import win32api\n",
        "\n",
        "win32api.LoadLibraryEx(sys.executable, 0, win32api.LOAD_LIBRARY_AS_DATAFILE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hRFZSCqJP9Xy"
      },
      "outputs": [],
      "source": [
        "from pywin32 import win32com.client as win32\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y95Uwg0yl0r9"
      },
      "outputs": [],
      "source": [
        "pip install pypiwin32 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xLsn83pVMqDw"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "SOURCE_URL = 'https://storage.googleapis.com/dm-turtle-recall/images.tar'\n",
        "IMAGE_DIR = './turtle_recall/images'\n",
        "TAR_PATH = os.path.join(IMAGE_DIR, os.path.basename(SOURCE_URL))\n",
        "EXPECTED_IMAGE_COUNT = 13891\n",
        "\n",
        "%sx mkdir --parents \"{IMAGE_DIR}\"\n",
        "if len(os.listdir(IMAGE_DIR)) != EXPECTED_IMAGE_COUNT:\n",
        "  %sx wget --no-check-certificate -O \"{TAR_PATH}\" \"{SOURCE_URL}\"\n",
        "  %sx tar --extract --file=\"{TAR_PATH}\" --directory=\"{IMAGE_DIR}\"\n",
        "  %sx rm \"{TAR_PATH}\"\n",
        "\n",
        "print(f'The total number of images is: {len(os.listdir(IMAGE_DIR))}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vFo2l5BYegGv"
      },
      "source": [
        "Read in the train, test, and sample submission CSV files as pandas dataframes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i_Q-Bqfn9Dry"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "import io\n",
        "import urllib.parse\n",
        "\n",
        "BASE_URL = 'https://storage.googleapis.com/dm-turtle-recall/'\n",
        "\n",
        "\n",
        "def read_csv_from_web(file_name):\n",
        "  url = urllib.parse.urljoin(BASE_URL, file_name)\n",
        "  content = requests.get(url).content\n",
        "  return pd.read_csv(io.StringIO(content.decode('utf-8')))\n",
        "\n",
        "\n",
        "# Read in csv files.\n",
        "train = read_csv_from_web('train.csv')\n",
        "test = read_csv_from_web('test.csv')\n",
        "sample_submission = read_csv_from_web('sample_submission.csv')\n",
        "\n",
        "# Convert image_location strings to lowercase.\n",
        "for df in [train, test]:\n",
        "  df.image_location = df.image_location.apply(lambda x: x.lower())\n",
        "  assert set(df.image_location.unique()) == set(['left', 'right', 'top'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g6DCF9uWGUOk"
      },
      "outputs": [],
      "source": [
        "train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iWrumN9fYTTL"
      },
      "outputs": [],
      "source": [
        "test.image_location.nunique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ePkEGRHYLfC"
      },
      "outputs": [],
      "source": [
        "train_top = train[train.image_location == 'top']\n",
        "train_left = train[train.image_location == 'left']\n",
        "train_right = train[train.image_location == 'right']\n",
        "\n",
        "print(train_top.shape, train_left.shape, train_right.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "52Q0kF8cGYfd"
      },
      "outputs": [],
      "source": [
        "test.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4dtCFz8kZIcD"
      },
      "outputs": [],
      "source": [
        "test_top = test[test.image_location == 'top']\n",
        "test_left = test[test.image_location == 'left']\n",
        "test_right = test[test.image_location == 'right']\n",
        "\n",
        "print(test_top.shape, test_left.shape, test_right.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LlAwtCwxmlEe"
      },
      "outputs": [],
      "source": [
        "train_left_ids = list(train_left.image_id)\n",
        "test_left_ids = list(test_left.image_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tDa9d7YQGaSl"
      },
      "outputs": [],
      "source": [
        "sample_submission.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eAJFgifWIn_w"
      },
      "outputs": [],
      "source": [
        "train.shape, test.shape, sample_submission.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x63GkXaiAOE8"
      },
      "source": [
        "How many unique turtles are in the training set?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DnTPq_zlG3VL"
      },
      "outputs": [],
      "source": [
        "print(f\"There are {train.turtle_id.nunique()} unique turtles in the train set.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mhAjSDRDhjb"
      },
      "source": [
        "How many images are there for each individual turtle in the training set?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Er5GYDiE5iB"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "train_images_per_turtle = pd.value_counts(train['turtle_id'])\n",
        "print('The mean number of training images per turtle is '\n",
        "      f'{round(np.mean(train_images_per_turtle), 2)}, '\n",
        "      f'and the median is {int(np.median(train_images_per_turtle))}.')\n",
        "sns.histplot(train_images_per_turtle)\n",
        "plt.xlabel('Images per train turtle')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J39WSJGCG_Et"
      },
      "source": [
        "We can plot the number of images per `turtle_id`:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CmTyBy7mG9I-"
      },
      "outputs": [],
      "source": [
        "images_per_turtle = pd.value_counts(train['turtle_id'])\n",
        "plt.figure(figsize=(3, 21))\n",
        "sns.barplot(x=images_per_turtle, y=images_per_turtle.index,\n",
        "            palette='Blues_r', orient='horizontal')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lbc3N6khS6qK"
      },
      "source": [
        "<a name=\"Approach\"></a>\n",
        "## 4. Approaching the modelling problem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdS9--dVVGWB"
      },
      "source": [
        "Since we want to match images to labels (classes), we are dealing with an **image classification** problem. Image classification is generally most successfully approached using deep **convolutional neural networks** (CNNs).\n",
        "\n",
        "### 4.1 Convolutional neural networks\n",
        "\n",
        "At the highest level, CNNs take images as inputs and return probabilities that the image belongs to each of the possible classes.\n",
        "\n",
        "In slightly more detail, CNNs hierarchically extract features from images using convolutional layers, which are usually followed by pooling layers that summarise the information in the extracted feature maps:\n",
        "- Lower CNN layers capture low-level image features (edges, blobs)\n",
        "- Layers deeper in the CNN capture higher-level features and objects (scale patterns, turtle eyes)\n",
        "- Fully connected layers can then consolidate these extracted patterns and objects\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://storage.googleapis.com/dm-turtle-recall/tutorial_images/cnn.png\" width=\"1100\"/>\n",
        "</p>\n",
        "\n",
        "We won't go into a detailed explanation of CNNs here; if you'd like to learn more about their inner workings, we recommend [CS231n: Convolutional Neural Networks for Visual Recognition](https://cs231n.github.io/).\n",
        "\n",
        "### 4.2 Transfer learning for image classification\n",
        "\n",
        "To approach this turtle face classification problem, we could specify a CNN architecture, initialise it, and proceed to train its parameters from scratch.\n",
        "\n",
        "An alternative approach, which is very popular in the computer vision (image processing) world, is called **transfer learning**:\n",
        "- Instead of training your model on your task from scratch, you first identify a model that has been **pre-trained** on some other image task\n",
        "- You can then adapt it (**\"fine-tune\"** it) to your specific task\n",
        "\n",
        "This is a popular approach because it often gets you two advantages:\n",
        "- **Improved performance**. The pre-training does a lot of the heavy lifting of learning to understand images. The most popular pre-trained models were trained on huge datasets for a long period of time, and you can simply piggy back on what they've learned.\n",
        "  - The features that these models learn are often reusable for many image-related tasks.\n",
        "  - This is because lower level visual features such as edges, colour blobs, simple composite shapes etc. are pervasive regardless of the specifics of the image task.\n",
        "- **Lower data and computational requirements**. Using a pre-trained model often means you don't need as much training data, time, or compute to reach a certain level of performance.\n",
        "\n",
        "There are plenty of pre-trained computer vision models available, the most popular probably being VGG, ResNet, EfficientNet, etc.\n",
        "\n",
        "Here, we will make use of a pre-trained ResNetX model made available via the **Haiku** library (a tool for building neural networks in **JAX**).\n",
        "\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://storage.googleapis.com/dm-turtle-recall/tutorial_images/jax.png\" width=\"900\"/>\n",
        "</p>\n",
        "\n",
        "The rest of this tutorial will implement an image classification neural network using JAX, which is a language or framework for numerical computation and deep learning that is frequently used by researchers and engineers at DeepMind. Since JAX is still less common than other tools such as Keras, Tensorflow, and PyTorch, the next section contains a quick introduction to some of the core features of JAX and Haiku.\n",
        "\n",
        "**Note**: It is not at all necessary to use JAX for this challenge, but we thought this might be a nice opportunity to introduce more people to this powerful and elegant way of writing machine learning code.\n",
        "\n",
        "If you are already familiar with JAX, feel free to skip the next section :)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kcR01uCUTRTh"
      },
      "source": [
        "<a name=\"Model\"></a>\n",
        "## 6. Fine-tuning a ResNet model using JAX and Haiku"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zdTVsZt3KydX"
      },
      "source": [
        "Time to get started with training our turtle facial recognition model!\n",
        "\n",
        "First, let's install and import some key libraries:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6cDPMJuaKvU5"
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "import jax.numpy as jnp  # JAX version of numpy with a very similar API.\n",
        "try:\n",
        "  import haiku as hk\n",
        "except ModuleNotFoundError:\n",
        "  !pip install dm-haiku\n",
        "  import haiku as hk\n",
        "try:\n",
        "  import optax\n",
        "except ModuleNotFoundError:\n",
        "  !pip install optax\n",
        "  import optax\n",
        "try:\n",
        "  import immutabledict\n",
        "except ModuleNotFoundError:\n",
        "  !pip install immutabledict\n",
        "  import immutabledict\n",
        "import functools\n",
        "from PIL import Image  # Image utilities.\n",
        "import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_zcw0kqfRor"
      },
      "source": [
        "Create three mappings and get the paths to the training set image files.\n",
        "1. `labels` : turtle ID --> unique integer labels\n",
        "1. `label_lookup` : unique integer labels --> turtle ID\n",
        "1. `image_to_turtle` :  image IDs to turtle IDs (training set only).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ak6pcvg-f7bF"
      },
      "outputs": [],
      "source": [
        "turtle_ids = sorted(np.unique(train.turtle_id)) + ['new_turtle']\n",
        "labels = dict(zip(turtle_ids, np.arange(len(turtle_ids))))\n",
        "label_lookup = {v: k for k, v in labels.items()}\n",
        "print(label_lookup)\n",
        "num_classes = len(labels) \n",
        "image_to_turtle = dict(zip(train.image_id, train.turtle_id))\n",
        "\n",
        "image_files_left = [os.path.join(IMAGE_DIR, f) for f in os.listdir(IMAGE_DIR)\n",
        "              if f.split('.')[0] in train_left.image_id.values]\n",
        "\n",
        "image_files_other = [os.path.join(IMAGE_DIR, f) for f in os.listdir(IMAGE_DIR)\n",
        "              if f.split('.')[0] in (train_right.image_id.values or train_top.image_id.values)]\n",
        "image_files = image_files_left + image_files_other\n",
        "\n",
        "image_ids = [os.path.basename(f).split('.')[0] for f in image_files]\n",
        "image_turtle_ids = [image_to_turtle[id] for id in image_ids]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esv-vvDGjLJ-"
      },
      "source": [
        "Load the training images into memory - takes a little while!\n",
        "\n",
        "*   Crops each image around the centre and resizes to `(224, 224)`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hX1qaHBVnqH7"
      },
      "outputs": [],
      "source": [
        "import PIL\n",
        "def crop_and_resize(pil_img, rotate=False):\n",
        "  \"\"\"Crops square from center of image and resizes to (224, 224).\"\"\"\n",
        "  if rotate:\n",
        "    # делаем все вправо\n",
        "    pil_img = pil_img.rotate(180)\n",
        "\n",
        "  w, h = pil_img.size\n",
        "  crop_size = min(w, h)\n",
        "  crop = pil_img.crop(((w - crop_size) // 2, (h - crop_size) // 2,\n",
        "                       (w + crop_size) // 2, (h + crop_size) // 2))\n",
        "  return crop.resize((224, 224))\n",
        "\n",
        "\n",
        "\n",
        "tqdm.tqdm._instances.clear()\n",
        "loaded_images = [crop_and_resize(Image.open(f)) for f in tqdm.tqdm(image_files)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A8_04m9w2j7x"
      },
      "outputs": [],
      "source": [
        "def crop_and_resize(pil_img):\n",
        "  \"\"\"Crops square from center of image and resizes to (224, 224).\"\"\"\n",
        "  w, h = pil_img.size\n",
        "  crop_size = min(w, h)\n",
        "  crop = pil_img.crop(((w - crop_size) // 2, (h - crop_size) // 2,\n",
        "                       (w + crop_size) // 2, (h + crop_size) // 2))\n",
        "  return crop.resize((224, 224))\n",
        "\n",
        "\n",
        "\n",
        "tqdm.tqdm._instances.clear()\n",
        "loaded_images = [crop_and_resize(Image.open(f)) for f in tqdm.tqdm(image_files)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3zqpuZef9P67"
      },
      "source": [
        "Define a function to get a random batch of data from the training images:\n",
        "\n",
        "\n",
        "*   Randomly select `batch_size` elements from the available images\n",
        "*   Get the labels for the selected images\n",
        "*   Optionally rebalance the dataset so that every label is sampled uniformly\n",
        "\n",
        "\n",
        "Returns the batch of images of shape `(batch_size, 224, 224, 3)` and the integer labels of shape `(batch_size)`\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q4M0boTqq2Xv"
      },
      "outputs": [],
      "source": [
        "probability_per_label = {\n",
        "    label: 1 / label_count / len(train_images_per_turtle)\n",
        "    for label, label_count in train_images_per_turtle.items()\n",
        "}\n",
        "\n",
        "probabilities = [\n",
        "    probability_per_label[image_turtle_id]\n",
        "    for image_turtle_id in image_turtle_ids\n",
        "]\n",
        "assert np.isclose(1., np.sum(probabilities))\n",
        "\n",
        "\n",
        "def get_batch(batch_size, rebalance=False):\n",
        "  if rebalance:\n",
        "    probs = probabilities\n",
        "  else:\n",
        "    probs = None\n",
        "  batch_image_idxs = np.random.choice(\n",
        "      len(image_files), size=batch_size, replace=False, p=probs)\n",
        "  input_images = [loaded_images[idx] for idx in batch_image_idxs]\n",
        "  image_labels = [labels[image_turtle_ids[idx]] for idx in batch_image_idxs]\n",
        "  return (jnp.stack([\n",
        "      jnp.asarray(im, dtype=jnp.float32) / 255. for im in input_images\n",
        "  ]), jnp.stack(image_labels).astype(jnp.int32))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mkUWDVRc_RCU"
      },
      "outputs": [],
      "source": [
        "batch_images, _ = get_batch(batch_size=32)\n",
        "\n",
        "_, axes = plt.subplots(nrows=4, ncols=8, figsize=(12, 6))\n",
        "axes = axes.flatten()\n",
        "for img, ax in zip(list(batch_images), axes):\n",
        "  ax.imshow(img)\n",
        "  ax.xaxis.set_visible(False)\n",
        "  ax.yaxis.set_visible(False)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X0jPPqj8isD2"
      },
      "source": [
        "###ResNet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46o57JYv-Vrx"
      },
      "source": [
        "Define our network: a ResNet50 model made available via the Haiku library. The output is of size `num_classes`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UFWrJX-4Wp4v"
      },
      "outputs": [],
      "source": [
        "@hk.without_apply_rng\n",
        "@hk.transform_with_state\n",
        "def resnet(x, is_training):\n",
        "  return hk.nets.ResNet50(\n",
        "      num_classes=num_classes, resnet_v2=True,\n",
        "      bn_config={'decay_rate': 0.9})(x, is_training)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WNW8BEYS_tQ-"
      },
      "source": [
        "Next we define our loss and update functions.\n",
        "\n",
        "The loss function computes the softmax cross entropy between the set of logits and labels and sums over the batch. We also add L2 regularisation on the model\n",
        "parameters to help to alleviate overfitting.\n",
        "\n",
        "The update function computes the gradients and updates the parameters using the `jax.grad` and `optax.apply_updates` utility functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J6LlSNAZ6P3e"
      },
      "outputs": [],
      "source": [
        "@functools.partial(jax.value_and_grad, has_aux=True)\n",
        "def loss_fn(params, state, inputs, labels):\n",
        "  predicted, new_state = net.apply(params, state, inputs, is_training=True)\n",
        "  predicted = jax.nn.log_softmax(predicted, axis=-1)\n",
        "  labels_one_hot = jax.nn.one_hot(labels, num_classes=num_classes)\n",
        "  loss = -(predicted * labels_one_hot).sum(axis=-1).mean()\n",
        "  loss = loss + l2_regularisation(params) * 0.005\n",
        "  return loss, new_state\n",
        "\n",
        "\n",
        "@jax.jit\n",
        "def update(params, state, opt_state, inputs, labels):\n",
        "  (loss, new_state), grads = loss_fn(params, state, inputs, labels)\n",
        "  updates, new_opt_state = opt.update(grads, opt_state)\n",
        "  new_params = optax.apply_updates(params, updates)\n",
        "  return new_params, new_state, new_opt_state, loss\n",
        "\n",
        "\n",
        "def l2_regularisation(params):\n",
        "  l2_norm = 0.\n",
        "  for module_name, module_params in params.items():\n",
        "    if 'batchnorm' not in module_name:\n",
        "      l2_norm += sum(\n",
        "          [jnp.sum(jnp.square(x)) for x in jax.tree_leaves(module_params)])\n",
        "  return l2_norm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-KqMOSdGeO63"
      },
      "source": [
        "Now that our forward pass, loss function, and update function are defined, let's load in some pre-trained weights from a ResNet50 model trained on ImageNet:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bm3z91v_eO-E"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "checkpoint_url = urllib.parse.urljoin(BASE_URL,\n",
        "                                      'resnet50_imagenet_checkpoint.pystate')\n",
        "checkpoint = pickle.loads(requests.get(checkpoint_url).content)\n",
        "\n",
        "# Get model params and state from the checkpoint.\n",
        "pretrained_params = checkpoint['experiment_module']['params']\n",
        "pretrained_state = checkpoint['experiment_module']['state']\n",
        "pretrained_opt_state = checkpoint['experiment_module']['opt_state']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3790jJW8MPu"
      },
      "source": [
        "Most of the parameters of the pre-trained model are the same, except for the final layer:\n",
        "- The pretrained ResNet50 model was trained to predict 1000 output classes\n",
        "- Our current classification task has 101 output classes\n",
        "\n",
        "This means we can use all of the pretrained parameters expect for those in the final layer, which will have to be learned from scratch.\n",
        "\n",
        "We'll also need to update the optimiser state `opt_state` to reflect this change in final layer params."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HGfzhTyN8yil"
      },
      "outputs": [],
      "source": [
        "import tree\n",
        "\n",
        "\n",
        "def update_params(path, values, scale_factor=-1e-5):\n",
        "  if path[-2:] == ('res_net50/~/logits', 'b'):\n",
        "    return scale_factor * jax.random.normal(\n",
        "        jax.random.PRNGKey(0), (num_classes,))\n",
        "  elif path[-2:] == ('res_net50/~/logits', 'w'):\n",
        "    return scale_factor * jax.random.normal(\n",
        "        jax.random.PRNGKey(0), (values.shape[0], num_classes))\n",
        "  else:\n",
        "    return values\n",
        "\n",
        "\n",
        "def update_opt_state(path, values):\n",
        "  if path[-2:] == ('res_net50/~/logits', 'b'):\n",
        "    return jnp.zeros((num_classes,))\n",
        "  elif path[-2:] == ('res_net50/~/logits', 'w'):\n",
        "    return jnp.zeros((values.shape[0], num_classes))\n",
        "  else:\n",
        "    return values\n",
        "\n",
        "\n",
        "pretrained_params = tree.map_structure_with_path(update_params,\n",
        "                                                 pretrained_params)\n",
        "pretrained_opt_state = tree.map_structure_with_path(update_opt_state,\n",
        "                                                    pretrained_opt_state)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FGug8dT8nh7r"
      },
      "source": [
        "Fine tune the model using the pretrained parameters to warm start the model (here, we are using the Adam optimiser from `optax`):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bHDuycDGjH4c"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "finetuned_losses = []\n",
        "n_steps = 400\n",
        "opt = optax.adam(1e-3)\n",
        "opt_state = opt.init(pretrained_params)\n",
        "\n",
        "net = resnet\n",
        "\n",
        "for step in range(n_steps):\n",
        "  batch_images, batch_labels = get_batch(batch_size)\n",
        "  pretrained_params, pretrained_state, opt_state, loss = update(\n",
        "      pretrained_params, pretrained_state, opt_state, batch_images,\n",
        "      batch_labels)\n",
        "  finetuned_losses.append(loss)\n",
        "  if step % 50 == 0:\n",
        "    print(f\"Loss at step {step}: {loss:.3f}.\", flush=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-_A4zFioH4h"
      },
      "source": [
        "Plot the learning curve from this run:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C7vWUImCoJtP"
      },
      "outputs": [],
      "source": [
        "plt.plot(np.arange(len(finetuned_losses)), finetuned_losses)\n",
        "plt.title(\"Fine-tuned model learning curve.\")\n",
        "plt.xlabel(\"Training steps\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0MjU2drkmzp"
      },
      "source": [
        "###CNN\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovvlnoEMdjxr"
      },
      "source": [
        "We can also train a simple CNN network from scratch:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BrLj9r6h9qG2"
      },
      "outputs": [],
      "source": [
        "@hk.without_apply_rng\n",
        "@hk.transform_with_state\n",
        "def simple_cnn(x, is_training):\n",
        "  def conv_block(x, channels, kernel):\n",
        "    x = hk.Conv2D(\n",
        "        output_channels=channels,\n",
        "        kernel_shape=kernel,\n",
        "        stride=2,\n",
        "        padding='SAME',\n",
        "        with_bias=True)(\n",
        "            x)\n",
        "    x = jax.nn.relu(x)\n",
        "    return x\n",
        "\n",
        "  for channels, kernel in zip([8, 16], [3, 5]):\n",
        "    x = conv_block(x, channels, kernel)\n",
        "  x = hk.Flatten()(x)\n",
        "  x = hk.Linear(256)(x)\n",
        "  x = jax.nn.relu(x)\n",
        "  x = hk.Linear(num_classes)(x)\n",
        "  return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vwmKhq30D_tJ"
      },
      "outputs": [],
      "source": [
        "# Initialise new network params, state, and optimiser from scratch.\n",
        "\n",
        "net = simple_cnn\n",
        "\n",
        "image_for_init, _ = get_batch(1)\n",
        "params, state = net.init(jax.random.PRNGKey(1), image_for_init, True)\n",
        "opt = optax.adam(3e-4)\n",
        "opt_state = opt.init(params)\n",
        "\n",
        "###################   МОЕ ГЕНИАЛЬНОЕ ИЗМЕНЕНИЕ ##########################\n",
        "batch_size = 64\n",
        "#########################################################################\n",
        "from_scratch_losses = []\n",
        "n_steps = 1000\n",
        "\n",
        "for step in range(n_steps):\n",
        "  batch_images, batch_labels = get_batch(batch_size, rebalance=True)\n",
        "  params, state, opt_state, loss = update(params, state, opt_state,\n",
        "                                          batch_images, batch_labels)\n",
        "  from_scratch_losses.append(loss)\n",
        "  if step % 50 == 0:\n",
        "    print(f\"Loss at step {step}: {loss:.3f}.\", flush=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymIOtThP_sDK"
      },
      "source": [
        "Compare the learning curves from the two models:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2fPS_k-r_uYK"
      },
      "outputs": [],
      "source": [
        "figure, axes = plt.subplots(ncols=2, sharey=True, figsize=(10, 4))\n",
        "\n",
        "axes[0].plot(np.arange(len(finetuned_losses)), finetuned_losses)\n",
        "axes[0].set_title(\"Fine-tuning a pretrained model\")\n",
        "axes[0].set(xlabel=\"Training steps\", ylabel=\"Loss\")\n",
        "axes[1].plot(np.arange(len(from_scratch_losses)), from_scratch_losses)\n",
        "axes[1].set_title(\"Training from scratch\")\n",
        "axes[1].set(xlabel=\"Training steps\", ylabel=\"Loss\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZNsUNk3_uam"
      },
      "source": [
        "### 7. Performing inference using trained models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQjH3J_seK4Z"
      },
      "source": [
        "We can make predictions on new examples using a trained model by passing the model parameters, state and (preprocessed) inputs to the `apply` function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DnUCLMwFHOPt"
      },
      "outputs": [],
      "source": [
        "@jax.jit\n",
        "def predict(params, state, inputs):\n",
        "  \"\"\"Forward pass of model with log softmaxed output.\"\"\"\n",
        "  predicted, _ = net.apply(params, state, inputs, is_training=False)\n",
        "  return jax.nn.log_softmax(predicted, axis=-1)\n",
        "\n",
        "\n",
        "def get_image_by_image_id(image_id):\n",
        "  \"\"\"Function to get a model-ready image given an image ID\"\"\"\n",
        "  all_image_files = os.listdir(IMAGE_DIR)\n",
        "  all_image_ids = [\n",
        "      os.path.basename(file).split('.')[0] for file in all_image_files\n",
        "  ]\n",
        "  if image_id not in all_image_ids:\n",
        "    raise ValueError(f'Could not find image with ID {image_id}')\n",
        "  image_filepath = all_image_files[all_image_ids.index(image_id)]\n",
        "  image = Image.open(os.path.join(IMAGE_DIR, image_filepath))\n",
        "  image = crop_and_resize(image)\n",
        "  return jnp.stack([jnp.asarray(image, dtype=jnp.float32) / 255.])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTgdDd0PDsI9"
      },
      "source": [
        "## 8. Generating test set predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P30YUCZ-3mjU"
      },
      "source": [
        "We can generate predictions for the entire test set by simply calling `predict` on each example. Let's first load the test images and apply the same cropping and resizing as before:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ciQ83LaAEA2p"
      },
      "outputs": [],
      "source": [
        "tqdm.tqdm._instances.clear()\n",
        "test_image_files = [os.path.join(IMAGE_DIR, f) for f in os.listdir(IMAGE_DIR)\n",
        "                    if f.split('.')[0] in test.image_id.values]\n",
        "test_image_ids = [os.path.basename(f).split('.')[0] for f in test_image_files]\n",
        "loaded_test_images = [crop_and_resize(Image.open(f)) for f in tqdm.tqdm(test_image_files)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iRho_jz330gB"
      },
      "source": [
        "The following utilities will perform batch inference and format the results. For submission we need a csv with an `image_id` column, and separate columns for each of our top 5 predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jl3sImdxwgOu"
      },
      "outputs": [],
      "source": [
        "import collections\n",
        "\n",
        "def batch_list(list_to_batch, batch_size):\n",
        "  \"\"\"Chunk up a list into batches, potentially with a smaller final batch.\"\"\"\n",
        "  return [list_to_batch[i:i + batch_size] for i in range(\n",
        "      0, len(list_to_batch), batch_size)]\n",
        "\n",
        "def predict_on_set(batched_image_ids,\n",
        "                   batched_images,\n",
        "                   params, state):\n",
        "  \"\"\"Returns top 5 predictions on batched images as a submission dataframe.\"\"\"\n",
        "  model_predictions = []\n",
        "\n",
        "  for ids, images in zip(batched_image_ids, batched_images):\n",
        "    # Stack images for batch inference.\n",
        "    images = jnp.stack(\n",
        "        [jnp.asarray(im, dtype=jnp.float32) / 255. for im in images])\n",
        "\n",
        "    # Make predictions and sort logits to find top 5 predictions.\n",
        "    logits = predict(params, state, images)\n",
        "    logits = jax.device_get(logits)\n",
        "    top_5_predictions = np.argsort(logits)[:, -5:][:, ::-1]\n",
        "    print(top_5_predictions)\n",
        "\n",
        "    # Format results.\n",
        "    for image_id, predictions in zip(ids, top_5_predictions):\n",
        "      row = {}\n",
        "      predicted_turtle_ids = [label_lookup[label] for label in predictions]\n",
        "      row['image_id'] = image_id\n",
        "      for prediction_idx, prediction in enumerate(predicted_turtle_ids):\n",
        "        row[f'prediction{prediction_idx + 1}'] = prediction\n",
        "      model_predictions.append(row)\n",
        "\n",
        "  return pd.DataFrame(model_predictions).set_index('image_id')\n",
        "\n",
        "# Batch up test images and IDs.\n",
        "############### ВНИМАНИМАНИЕ! ЗДЕСЬ ТОЖЕ ПРИШЛОСЬ ПОМЕНЯТЬ! ####################\n",
        "eval_batch_size = 64\n",
        "################################################################################\n",
        "test_batched_images = batch_list(loaded_test_images, eval_batch_size)\n",
        "test_batched_image_ids = batch_list(test_image_ids, eval_batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1-YxvWUDxN_E"
      },
      "outputs": [],
      "source": [
        "net = simple_cnn\n",
        "predictions_from_scratch = predict_on_set(test_batched_image_ids,\n",
        "                                          test_batched_images,\n",
        "                                          params, state)\n",
        "net = resnet\n",
        "predictions_from_pretrained = predict_on_set(test_batched_image_ids,\n",
        "                                             test_batched_images,\n",
        "                                             pretrained_params,\n",
        "                                             pretrained_state)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PZcD5MAgMj7f"
      },
      "outputs": [],
      "source": [
        "predictions_from_scratch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UWB1fahrZIvZ"
      },
      "source": [
        "<a name=\"Submit\"></a>\n",
        "## 9. Submitting our predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dTRlGxo4Amj"
      },
      "source": [
        "Now our predictions are ready, we can save them to file and download them, ready to submit to Zindi:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L1UzR9gq4FGE"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "\n",
        "predictions_from_scratch.to_csv('submission.csv')\n",
        "files.download('submission.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJWEXsLwfUy-"
      },
      "source": [
        "### The evaluation metric: Mean Average Precision (`MAP@5`)\n",
        "\n",
        "We are using Mean Average Precision at 5, where 5 refers to the number of predictions submitted for each turtle.\n",
        "\n",
        "\n",
        "#### Precision @k (`P@k`)\n",
        "\n",
        "- Defined as `true_positives_in_top_k_predictions / k `, this captures how many relevant items are present in the top `k` recommendations of your system.\n",
        "\n",
        "- For example, let's assume the the prediction of one row is as follows:\n",
        "\n",
        "  ```actual = \"t_id_ROFhVsy2\"```\n",
        "\n",
        "  ``` predicted = [\"t_id_ROFhVsy2\", \"t_id_UVQa4BMz\", \"t_id_a4VYrmyA\", \"new_turtle\", \"t_id_4ZfTUmwL\"]```\n",
        "\n",
        "  Then, for different values of `k`:\n",
        "\n",
        "  `P@1  = 1/1 `\n",
        "\n",
        "  `P@2 = 1/2 `\n",
        "\n",
        "  `P@3 = 1/3`\n",
        "\n",
        "  `P@4 = 1/4`\n",
        "\n",
        "  `P@5 = 1/5`\n",
        "\n",
        "  <br>\n",
        "\n",
        "#### Average precision @ k (`AP@k`)\n",
        "- Defined as the mean of `P@i` for `i=1, ..., K`.\n",
        "\n",
        "<br>\n",
        "\n",
        "#### Mean Average Precision @k (`MAP@K`)\n",
        "- Defined as the mean of the `AP@K` for all the turtles.\n",
        "\n",
        "- For our metric, `MAP@5`: sum `AP@5` for all the turtles and divide that value by the number of turtles"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lv2m6J041L-e"
      },
      "source": [
        "Here are functions that you can use to calculate the `AP@5` and `MAP@5` for a given label and list of predictions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YWZ6OIUW1YDn"
      },
      "outputs": [],
      "source": [
        "def apk(actual, predicted, k=5):\n",
        "  \"\"\"Computes the average precision at k.\n",
        "\n",
        "  Args:\n",
        "    actual: The turtle ID to be predicted.\n",
        "    predicted : A list of predicted turtle IDs (order does matter).\n",
        "    k : The maximum number of predicted elements.\n",
        "\n",
        "  Returns:\n",
        "    The average precision at k.\n",
        "  \"\"\"\n",
        "  if len(predicted) > k:\n",
        "    predicted = predicted[:k]\n",
        "\n",
        "  score = 0.0\n",
        "  num_hits = 0.0\n",
        "\n",
        "  for i, p in enumerate(predicted):\n",
        "    if p == actual and p not in predicted[:i]:\n",
        "      num_hits += 1.0\n",
        "      score += num_hits / (i + 1.0)\n",
        "\n",
        "  return score\n",
        "\n",
        "\n",
        "def mapk(actual, predicted, k=5):\n",
        "  \"\"\" Computes the mean average precision at k.\n",
        "\n",
        "    The turtle ID at actual[i] will be used to score predicted[i][:k] so order\n",
        "    matters throughout!\n",
        "\n",
        "    actual: A list of the true turtle IDs to score against.\n",
        "    predicted: A list of lists of predicted turtle IDs.\n",
        "    k: The size of the window to score within.\n",
        "\n",
        "    Returns:\n",
        "      The mean average precision at k.\n",
        "  \"\"\"\n",
        "  return np.mean([apk(a, p, k) for a, p in zip(actual, predicted)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fRXRDtflz2xP"
      },
      "outputs": [],
      "source": [
        "predictions = predictions_from_scratch[[\n",
        "    \"prediction1\", \"prediction2\", \"prediction3\", \"prediction4\", \"prediction5\"\n",
        "]]\n",
        "y_predict = predictions.values.tolist()\n",
        "\n",
        "# We don't actually know the true labels for the test set, so for the purposes\n",
        "# of demonstration we just assume that all of the images in the test set are of\n",
        "# a single turtle:\n",
        "assumed_y = [\"t_id_d6aYXtor\"] * len(y_predict)\n",
        "\n",
        "mapk_result = mapk(assumed_y, y_predict, k=5)\n",
        "print(\"With made up test set labels, our mapk with k=5 is\", mapk_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0zA8pLH36rd"
      },
      "source": [
        "### Generalisability prize\n",
        "Please note that there is an additional prize for generalisability: The likelihood of the approach and algorithm being able to generalise beyond the challenge dataset without frequent re-training, taking into account the approach and algorithm used."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sikpOFzXZIyG"
      },
      "source": [
        "<a name=\"Suggestions\"></a>\n",
        "## 8. Suggestions for improving the model\n",
        "\n",
        "This tutorial provides a jumping off point for this task, but there are plenty of improvements you might consider making to the model presented in this colab. A few possible directions to explore:\n",
        "- **Data-related**\n",
        "  - Preprocessing the dataset in various ways - cropping, colour-correction, etc.\n",
        "  - Augmenting the dataset - flipping, rotating, tinting, etc. the images to increase data size and potentially improve model generalisation\n",
        "  - Making use of the extra images, listed in `extra_images.csv`, provided in the dataset\n",
        "- **Model-related**\n",
        "  - Outputting 'new_turtle' when the model is particularly uncertain\n",
        "  - Trying different pre-trained models\n",
        "  - Playing around with the model architecture\n",
        "  - Trying model ensembling\n",
        "  - Hyperparameter tuning\n",
        "  - Other regularisation approaches"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRShUBH3oMhr"
      },
      "source": [
        "Thanks for reading, hope you enjoyed this tutorial and we're looking forward to seeing your entries!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4wxw2ySGTm24"
      },
      "source": [
        "<a name=\"Legal\"></a>\n",
        "## 9. License and Disclaimer\n",
        "\n",
        "This is not an officially-supported Google product.\n",
        "\n",
        "Copyright 2021 DeepMind Technologies Limited.\n",
        "\n",
        "This notebook and code is licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at https://www.apache.org/licenses/LICENSE-2.0.\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.\n",
        "\n",
        "\n",
        "**Model Parameters License**\n",
        "\n",
        "The pre-trained model parameters are made available under the terms of the Creative Commons Attribution 4.0 International (CC BY 4.0) license. You can find details at: https://creativecommons.org/licenses/by/4.0/legalcode\n",
        "\n",
        "**Data Set**\n",
        "\n",
        "The data set of turtle images and associated labels have been provided by Zindi and Local Ocean Conservation.\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "ResNet_hw.ipynb",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.1 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.1"
    },
    "vscode": {
      "interpreter": {
        "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
